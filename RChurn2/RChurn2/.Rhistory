t <- list()
t <- c(t,c(0,1,2))
t
t <- c(t,c(0,1,2))
t
t <- list()
toString( t)
t
t <- c(t,c(0,1,2))
t <- c(t,c(0,1,2))
toString( t)
t <- list()
t[[1]] <- c(1,2,3,4)
t[[2]] <- c(1,8,16,32,64)
t
toString(t)
toString(t[[1]])
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
librar
library(keras)
install.packages("devtools")
install
install.packages("devtools")
devtools::install_github("r-lib/processx")
devtools::install_github("r-lib/processx")
install.packages("processx")
install.packages("debugme")
source("https://install-github.me/gaborcsardi/debugme")
install.packages("processx")
devtools::install_github("r-lib/processx")
devtools::install_github("rstudio/keras")
library(keras)
?install_tensorflow
install_tensorflow
model <- Sequential()
model <- keras_model_sequential()
install_tensorflow_conda
install_tensorflow(gpu=T)
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
library(reticulate)
use_condaenv("r-tensorflow")
model <- kera
model <- keras_model_sequential()
tf_config()
reticulate::py_config()
use_condaenv("r-tensorflow")
reticulate::py_config()
use_condaenv("r-tensorflow")
tf_config()
use_condaenv("r-tensorflow", required=TRUE)
use_condaenv("r-tensorflow")
tf_config()
reticulate::py_config()
tf_config()
use_condaenv
conda_list
conda_list()
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
n
n
n
n
reticulate::use_condaenv("r-tensorflow")
reticulate::con
tf_config()
conda_list()
use_condaenv("r-tensorflow")
tf_config()
library(keras)
install_tensorflow
install_tensorflow(method="conda", gpu=TRUE)
m <- keras_model_sequential()
m <- keras_model_sequential()
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
logger(sprintf("Best train Recall: %.3f Kappa: %.3f for %s",     best_train_recall, best_train_kappa, toString(best_train_layers))) logger(sprintf("Best test  Recall: %.3f Kappa: %.3f for %s",     best_test_recall, best_test_kappa, toString(best_test_layers)))
nn_res
conf_mats[[3]]
v <- toString(conf_mats[[3]])
v
data.frame(nn_res,nn_res)
rbind(nn_res,nn_res)
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
d <- data.frame()
d["test"] <- 1
d <- data.frame(a=c(1))
d["b"]<- 22
d
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
n
results
results["tr"] <- 1
Q
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
nn_res["TrainRecall"]
nn_res
d <- data.frame(1,2,3,4)
colnames(d) <- c("ä","b","c","d")
d
d["b"]
ä <- d["b"]
a <- d["b"]
a
sprintf("%d",a)
sprintf("%d",a)
sprintf("%f",a)
sprintf("%.1f",a)
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2_HPC_nn_only.R")
conf
conf_mats[[7]]
conf_mats[[1]]
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
logger(sprintf("Total %d churned out of %d customers", nr_churn, nrow(df_full))) logger("Computing confusion matrix...") cnf <- confusionMatrix(pred_full, y_full, positive = "1") print(cnf)
yhtrain <- predict(final_)
yhtrain <- predict(final_model, as.matrix(X_train))
trainpred <- thtrain>0.4
trainpred <- yhtrain>0.4
confusionMatrix(trainpre,y_train, positive = "1")
confusionMatrix(trainpred,y_train, positive = "1")
trainpred <- as.inte
trainpred <- as.integer(trainpred)
confusionMatrix(trainpred,y_train, positive = "1")
cluster_fields <- c("R", "F", "M") df_cl <- FullClustering(df_full[, cluster_fields], cluster_fields,                                n_centers = 4, n_micros = 100,                                log_list = c(0, 1, 1))
df_cl[:10,]
df_cl[1:10,]
plot(df_cl$sub2_F,df_cl$sub3_M)
dd <- unique(df_cl[,c("sub2_F","sub3_M")])
nrow(dd)
head(dd)
dd <- unique(df_cl[,c("sub2_F","sub3_M","SCLID")])
head(dd)
dd <- unique(df_cl[,c("sub2_F","sub3_M","IDCLU")])
plot(dd$sub2_F,dd$sub3_M,col=dd$IDCLU)
heaD(df_cl)
head(df_cl)
t <- df_cl[,c("P1_R","P2_F","P3_M")]
describe(t)
summary(t)
qplot
summary(dd)
dd <- unique(df_cl[,c("sub2_F","sub3_M","IDCLU","SCLID")])
head(dd[dd$IDCLU==4,])
head(dd[dd$IDCLU==1,])
head(dd[dd$IDCLU==2,])
head(df_cl)
unique(df_cl[,c("cen1_R","cen2_F","cen3_M","SCORE","IDCLU")])
summary(df_cl$M)
summary(df_cl$P3_M)
unique(df_cl[,c("cen1_R","cen2_F","cen3_M","SCORE","IDCLU","SUBCL")])
FullClustering <- function(df, column_list, n_centers, n_micros,                            log_list, log_all_columns = FALSE) {   centroid_labels <- c()   subcluster_column_list <- c()   for (i in 1:length(column_list)) {     col <- column_list[i]     new_col <- paste0("cen", i, "_", substr(col, 1, 3))     sub_new_col <- paste0("sub", i, "_", substr(col, 1, 3))     centroid_labels <- c(centroid_labels, new_col)     subcluster_column_list <- c(subcluster_column_list, sub_new_col)   }   # base clusterization model   clust_column <- "IDCLU" # IMPORTANT: segment ID (sorted from worst to best)   label_column <- "LABEL" # IMPORTANT: inferred label for ID   score_column <- "SCORE" # IMPORTANT: score of each segment   loss_column <- "LOSSC" # loss of the model   subcl_column <- "SCLID" # IMPORTANT: ID of the subcluster (sorted from worst to best)   nrcus_column <- "NR_CL" # number of observation in main segment   tempc_column <- "TMPCL" # temporary segment - should be removed at cleanup   temp_scl_col <- "TMPSC" # temporary subcluster - should be removed at cleanup   subscore_col <- "SUBSC" # IMPORTANT: subcluster score   tsnex_column <- "TSNEX" # IMPORTANT: t-SNE X attribute   tsney_column <- "TSNEY" # IMPORTANT: t-SNE Y attribute   micro_fields <- c(cluster_column, subcl_column,                     centroid_labels, subcluster_column_list,                     label_column, score_column,                     tsnex_column, tsney_column)   segment_labels <- c("1-Very Low", "2-Low", "3-Average", "4-Good", "5-Best")   norm_columns <- c()   nr_input_fields <- length(column_list)   logger(sprintf("Normalising %d columns...", nr_input_fields))   for (i in 1:nr_input_fields) {     col_name <- column_list[i]     new_col <- paste0("P", i, "_", substr(col_name, 1, 3))     df[, new_col] <- df[, col_name] #copy data     norm_columns <- c(norm_columns, new_col)     is_log = 0     if (length(log_list) >= i)       is_log <- log_list[i]     if ((is_log == 1) || (log_all_columns)) {       below_zero_idxs <- (df[, new_col] <= 0)       EXP_BELOW_ZERO <- FALSE       if (sum(below_zero_idxs) > 0) {         if (EXP_BELOW_ZERO) {           df[below_zero_idxs, new_col] <-             exp(df[below_zero_idxs, new_col]) * 1e-3         } else {           fdelta <- min(df[below_zero_idxs, new_col])           df[below_zero_idxs, new_col] <-             (df[below_zero_idxs, new_col] - fdelta + 1) * 1e-5         }       }       df[, new_col] <- log(df[, new_col])     }     min_c <- min(df[, new_col])     max_c <- max(df[, new_col])     df[, new_col] <- (df[, new_col] - min_c) / (max_c - min_c)   }   logger(sprintf("Main clustering on data with shape: (%s)...\n",                     toString(dim(df[, norm_columns]))))   t0 <- proc.time()   timeit("Main kmeans: ",          trained_model <- CustomKMeans(            x = df[, norm_columns],            centers = n_centers, nstart = 30))   df[, tempc_column] <- trained_model$cluster   df_centers <- trained_model$center   scores <- rowMeans(df_centers)   TScore <- trained_model$tot.withinss   df_centers <- data.frame(df_centers, scores, TScore)   colnames(df_centers) <- c(centroid_labels, score_column, loss_column)   df_centers[, tempc_column] <- 1:nrow(df_centers)   df_centers <- df_centers[order(df_centers[, score_column]),]   df_centers[, clust_column] <- 1:nrow(df_centers)   df_centers[, label_column] <- segment_labels[1:n_centers]   logger("Merging...\n")   timeit(" Merge", df <- merge(df, df_centers, by = tempc_column))   clusterList <- unique(df[, clust_column])   n_cl <- length(clusterList)   for (i in 1:n_cl) {     cl <- clusterList[i]     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])   }   df_downsampled <- data.frame()   # generate sub-clusters for each cluster   for (i in 1:n_cl) {     cl <- clusterList[i]     logger(sprintf("Subclustering cluster %d...\n", cl))     timeit(" Subcluster kmeans: ",            cl_model <- CustomKMeans(              df[df[, clust_column] == cl, norm_columns],              centers = n_micros, nstart = 10))     df_subcenters <- cl_model$center     scores <- rowMeans(df_subcenters)     df_subcenters <- data.frame(df_subcenters, scores)     colnames(df_subcenters) <- c(subcluster_column_list, subscore_col)     df_subcenters[, clust_column] <- cl     df_subcenters[, temp_scl_col] <- 1:nrow(df_subcenters)     df_subcenters <- df_subcenters[order(df_subcenters[, subscore_col]),]     df_subcenters[, subcl_column] <- 1:nrow(df_subcenters)     df[df[, clust_column] == cl, temp_scl_col] <- cl_model$cluster     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])     df_downsampled <- rbind(df_downsampled, df_subcenters)   }   library(Rtsne)   logger("Applying t-SNE...\n")   timeit(" t-SNE ",          rtsne_res <- Rtsne(as.matrix(df_downsampled[, subcluster_column_list]),                             check_duplicates = FALSE,                             pca = TRUE))   df_downsampled[, tsnex_column] <- rtsne_res$Y[, 1]   df_downsampled[, tsney_column] <- rtsne_res$Y[, 2]   timeit("Final Merge ",          df <- merge(df, df_downsampled, by = c(clust_column, temp_scl_col)))   dfa <- data.frame(df_centers[, c(label_column, clust_column, score_column,                                    nrcus_column, centroid_labels)])   t1 <- proc.time()   elapsed <- t1[3] - t0[3]   logger(sprintf("Total processing time %.2f min\n", elapsed / 60))   result_list = list("dataframe" = df, "fields" = micro_fields)   return(result_list) }
results <- FullClustering(df_full[, cluster_fields], cluster_fields,                                n_centers = 4, n_micros = 100,                                log_list = c(0, 1, 1))
FullClustering <- function(df, column_list, n_centers, n_micros,                            log_list, log_all_columns = FALSE) {   centroid_labels <- c()   subcluster_column_list <- c()   for (i in 1:length(column_list)) {     col <- column_list[i]     new_col <- paste0("cen", i, "_", substr(col, 1, 3))     sub_new_col <- paste0("sub", i, "_", substr(col, 1, 3))     centroid_labels <- c(centroid_labels, new_col)     subcluster_column_list <- c(subcluster_column_list, sub_new_col)   }   # base clusterization model   clust_column <- "IDCLU" # IMPORTANT: segment ID (sorted from worst to best)   label_column <- "LABEL" # IMPORTANT: inferred label for ID   score_column <- "SCORE" # IMPORTANT: score of each segment   loss_column <- "LOSSC" # loss of the model   subcl_column <- "SCLID" # IMPORTANT: ID of the subcluster (sorted from worst to best)   nrcus_column <- "NR_CL" # number of observation in main segment   tempc_column <- "TMPCL" # temporary segment - should be removed at cleanup   temp_scl_col <- "TMPSC" # temporary subcluster - should be removed at cleanup   subscore_col <- "SUBSC" # IMPORTANT: subcluster score   tsnex_column <- "TSNEX" # IMPORTANT: t-SNE X attribute   tsney_column <- "TSNEY" # IMPORTANT: t-SNE Y attribute   micro_fields <- c(clust_column, subcl_column,                     centroid_labels, subcluster_column_list,                     label_column, score_column,                     tsnex_column, tsney_column)   segment_labels <- c("1-Very Low", "2-Low", "3-Average", "4-Good", "5-Best")   norm_columns <- c()   nr_input_fields <- length(column_list)   logger(sprintf("Normalising %d columns...", nr_input_fields))   for (i in 1:nr_input_fields) {     col_name <- column_list[i]     new_col <- paste0("P", i, "_", substr(col_name, 1, 3))     df[, new_col] <- df[, col_name] #copy data     norm_columns <- c(norm_columns, new_col)     is_log = 0     if (length(log_list) >= i)       is_log <- log_list[i]     if ((is_log == 1) || (log_all_columns)) {       below_zero_idxs <- (df[, new_col] <= 0)       EXP_BELOW_ZERO <- FALSE       if (sum(below_zero_idxs) > 0) {         if (EXP_BELOW_ZERO) {           df[below_zero_idxs, new_col] <-             exp(df[below_zero_idxs, new_col]) * 1e-3         } else {           fdelta <- min(df[below_zero_idxs, new_col])           df[below_zero_idxs, new_col] <-             (df[below_zero_idxs, new_col] - fdelta + 1) * 1e-5         }       }       df[, new_col] <- log(df[, new_col])     }     min_c <- min(df[, new_col])     max_c <- max(df[, new_col])     df[, new_col] <- (df[, new_col] - min_c) / (max_c - min_c)   }   logger(sprintf("Main clustering on data with shape: (%s)...\n",                     toString(dim(df[, norm_columns]))))   t0 <- proc.time()   timeit("Main kmeans: ",          trained_model <- CustomKMeans(            x = df[, norm_columns],            centers = n_centers, nstart = 30))   df[, tempc_column] <- trained_model$cluster   df_centers <- trained_model$center   scores <- rowMeans(df_centers)   TScore <- trained_model$tot.withinss   df_centers <- data.frame(df_centers, scores, TScore)   colnames(df_centers) <- c(centroid_labels, score_column, loss_column)   df_centers[, tempc_column] <- 1:nrow(df_centers)   df_centers <- df_centers[order(df_centers[, score_column]),]   df_centers[, clust_column] <- 1:nrow(df_centers)   df_centers[, label_column] <- segment_labels[1:n_centers]   logger("Merging...\n")   timeit(" Merge", df <- merge(df, df_centers, by = tempc_column))   clusterList <- unique(df[, clust_column])   n_cl <- length(clusterList)   for (i in 1:n_cl) {     cl <- clusterList[i]     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])   }   df_downsampled <- data.frame()   # generate sub-clusters for each cluster   for (i in 1:n_cl) {     cl <- clusterList[i]     logger(sprintf("Subclustering cluster %d...\n", cl))     timeit(" Subcluster kmeans: ",            cl_model <- CustomKMeans(              df[df[, clust_column] == cl, norm_columns],              centers = n_micros, nstart = 10))     df_subcenters <- cl_model$center     scores <- rowMeans(df_subcenters)     df_subcenters <- data.frame(df_subcenters, scores)     colnames(df_subcenters) <- c(subcluster_column_list, subscore_col)     df_subcenters[, clust_column] <- cl     df_subcenters[, temp_scl_col] <- 1:nrow(df_subcenters)     df_subcenters <- df_subcenters[order(df_subcenters[, subscore_col]),]     df_subcenters[, subcl_column] <- 1:nrow(df_subcenters)     df[df[, clust_column] == cl, temp_scl_col] <- cl_model$cluster     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])     df_downsampled <- rbind(df_downsampled, df_subcenters)   }   library(Rtsne)   logger("Applying t-SNE...\n")   timeit(" t-SNE ",          rtsne_res <- Rtsne(as.matrix(df_downsampled[, subcluster_column_list]),                             check_duplicates = FALSE,                             pca = TRUE))   df_downsampled[, tsnex_column] <- rtsne_res$Y[, 1]   df_downsampled[, tsney_column] <- rtsne_res$Y[, 2]   timeit("Final Merge ",          df <- merge(df, df_downsampled, by = c(clust_column, temp_scl_col)))   dfa <- data.frame(df_centers[, c(label_column, clust_column, score_column,                                    nrcus_column, centroid_labels)])   t1 <- proc.time()   elapsed <- t1[3] - t0[3]   logger(sprintf("Total processing time %.2f min\n", elapsed / 60))   result_list = list("dataframe" = df, "fields" = micro_fields)   return(result_list) }
results <- FullClustering(df_full[, cluster_fields], cluster_fields,                                n_centers = 4, n_micros = 100,                                log_list = c(0, 1, 1))
head(df_micros)
df_clf <- results$dataframe fields <- results$fields df_micros <- unique(df_clf[, fields])
head(df_micro)
head(df_micro)
head(df_micros)
nrow(df_micros)
plot(df_micros$sub)
plot(df_micros$sub1_R,df_micros$sub2_F,col=df_micros$SCLID)
plot(df_micros$sub1_R,df_micros$sub2_F,col=df_micros$IDCLU)
summary(df_cl)
df_cl["PRED_CHURN"] <- pred_full df_churn <- df[df["PRED_CHURN"] == 1,]
df_cl$P
summary(df_cl$PRED_CHURN)
nrow(df_cl)
df_churn <- df_cl[df_cl["PRED_CHURN"] == 1,]
nrow(df_churn)
ncol(df_churn)
normed_cols <- c("P1_R","P2_F","P3_M")
normed_cols
TestClustering(df_churn[, normed_cols], normed_cols)
library(ggplot2)
f1 <- "sub1_R"
f2 <- "sub2_F"
churn_clust <- FullClustering(df_churn[, cluster_fields], cluster_fields,                                      n_centers = 4, n_micros = 25,                                      log_list = c(0, 1, 1)) df_churn_cl <- churn_clust$dataframe
head(df_churn_cl)
seg <- "IDCLU"
plot1 <- qplot(df_micros[, f1], dataset[, f2],                shape = dataset[, seg],                color = dataset[, seg],                size = I(5.0),                alpha = 0.5) plot1 <- plot1 + labs(x = f1, y = f2)
plot1
plot1 <- qplot(df_micros[, f1], dataset[, f2],                shape = df_micros[, seg],                color = df_micros[, seg],                size = I(5.0),                alpha = 0.5) plot1 <- plot1 + labs(x = f1, y = f2)
plot1
PlotClustering <- function(df_micro, field1, field2, seg_field, initial_plot = NULL) {   if (!is.null(initial_plot)) {     current_plot <- initial_plot   }   current_plot <- qplot(df_micro[, field1], df_micro[, field1],                  shape = as.factor(df_micro[, seg_field]),                  color = as.factor(df_micro[, seg_field]),                  size = I(5.0),                  alpha = 0.5)   current_plot <- plot1 + labs(x = field1, y = field2)   return(current_plot) }
plot1 <- PlotClustering(df_micros, f1, f2, seg)
plot1
PlotClustering <- function(df_micro, field1, field2, seg_field, initial_plot = NULL) {   if (!is.null(initial_plot)) {     current_plot <- initial_plot   }   df_micro[, seg_field] <- as.factor(df_micro[, seg_field])   current_plot <- qplot(df_micro[, field1], df_micro[, field1],                  shape = df_micro[, seg_field],                  color = df_micro[, seg_field],                  size = I(5.0),                  alpha = 0.5)   current_plot <- plot1 + labs(x = field1, y = field2)   return(current_plot) }
plot1 <- PlotClustering(df_micros, f1, f2, seg)
plot1
seg
df_micros$IDCLU <- as.factor(df_micros$IDCLU)
summary(df_micros)
qplot(df_micros$sub1_R,df_micros$sub2_F, shape=df_micros$IDCLU)
qplot(df_micros$sub1_R,df_micros$sub2_F, shape=df_micros$IDCLU, color=df_micros$ID)
qplot(df_micros$sub1_R,df_micros$sub2_F, shape=df_micros[,"IDCLU"], color=df_micros[,"IDCLU"])
PlotClustering <- function(df_micro, field1, field2, seg_field, initial_plot = NULL) {   if (!is.null(initial_plot)) {     current_plot <- initial_plot   }   df_micro[, seg_field] <- as.factor(df_micro[, seg_field])   current_plot <- qplot(df_micro[, field1], df_micro[, field1],                  shape = df_micro[, seg_field],                  color = df_micro[, seg_field],                  size = I(5.0),                  alpha = 0.5)   current_plot <- plot1 + labs(x = field1, y = field2)   return(current_plot) }
PlotClustering(df_micros,"sub1_R","sub2_F", "IDCLU")
PlotClu
PlotClustering
PlotClustering <- function(df_micro, field1, field2, seg_field, initial_plot = NULL) {   if (!is.null(initial_plot)) {     current_plot <- initial_plot   }   df_micro[, seg_field] <- as.factor(df_micro[, seg_field])   current_plot <- qplot(df_micro[, field1], df_micro[, field1],                  shape = df_micro[, seg_field],                  color = df_micro[, seg_field],                  size = I(5.0),                  alpha = 0.5)   current_plot <- current_plot + labs(x = field1, y = field2)   return(current_plot) }
PlotClustering(df_micros,"sub1_R","sub2_F", "IDCLU")
PlotClustering <- function(df_micro, field1, field2, seg_field, initial_plot = NULL) {   if (!is.null(initial_plot)) {     current_plot <- initial_plot   }   df_micro[, seg_field] <- as.factor(df_micro[, seg_field])   current_plot <- qplot(df_micro[, field1], df_micro[, field2],                  shape = df_micro[, seg_field],                  color = df_micro[, seg_field],                  size = I(5.0),                  alpha = 0.5)   current_plot <- current_plot + labs(x = field1, y = field2)   return(current_plot) }
plot1 <- PlotClustering(df_micros, field1 = f1, field2 = f2, seg_field = seg)
plot1
nrow(df_churn_micros)
df_churn_cl
df_churn_micros <- unique(df_churn_cl[, fields])
dim(df_churn_micros)
head(df_churn_micros)
SimpleClustering <- function(df, column_list, n_centers, n_micros)   ##   ## clustering without any column scaling/norming   ##   {   centroid_labels <- c()   subcluster_column_list <- c()   for (i in 1:length(column_list)) {     col <- column_list[i]     new_col <- paste0("cen", i, "_", substr(col, 1, 3))     sub_new_col <- paste0("sub", i, "_", substr(col, 1, 3))     centroid_labels <- c(centroid_labels, new_col)     subcluster_column_list <- c(subcluster_column_list, sub_new_col)   }   # base clusterization model   clust_column <- "IDCLU" # IMPORTANT: segment ID (sorted from worst to best)   label_column <- "LABEL" # IMPORTANT: inferred label for ID   score_column <- "SCORE" # IMPORTANT: score of each segment   loss_column <- "LOSSC" # loss of the model   subcl_column <- "SCLID" # IMPORTANT: ID of the subcluster (sorted from worst to best)   nrcus_column <- "NR_CL" # number of observation in main segment   tempc_column <- "TMPCL" # temporary segment - should be removed at cleanup   temp_scl_col <- "TMPSC" # temporary subcluster - should be removed at cleanup   subscore_col <- "SUBSC" # IMPORTANT: subcluster score   tsnex_column <- "TSNEX" # IMPORTANT: t-SNE X attribute   tsney_column <- "TSNEY" # IMPORTANT: t-SNE Y attribute   micro_fields <- c(clust_column, subcl_column,                     centroid_labels, subcluster_column_list,                     label_column, score_column,                     tsnex_column, tsney_column)   segment_labels <- c("1-Very Low", "2-Low", "3-Average", "4-Good", "5-Best")   norm_columns <- column_list   nr_input_fields <- length(column_list)   logger(sprintf("Main clustering on data with shape: (%s)...\n",                     toString(dim(df[, norm_columns]))))   t0 <- proc.time()   timeit(" Main kmeans: ",          trained_model <- CustomKMeans(            x = df[, norm_columns],            centers = n_centers, nstart = 30))   df[, tempc_column] <- trained_model$cluster   df_centers <- trained_model$center   scores <- rowMeans(df_centers)   TScore <- trained_model$tot.withinss   df_centers <- data.frame(df_centers, scores, TScore)   colnames(df_centers) <- c(centroid_labels, score_column, loss_column)   df_centers[, tempc_column] <- 1:nrow(df_centers)   df_centers <- df_centers[order(df_centers[, score_column]),]   df_centers[, clust_column] <- 1:nrow(df_centers)   df_centers[, label_column] <- segment_labels[1:n_centers]   logger("Merging...\n")   timeit(" Merge", df <- merge(df, df_centers, by = tempc_column))   clusterList <- unique(df[, clust_column])   n_cl <- length(clusterList)   for (i in 1:n_cl) {     cl <- clusterList[i]     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])   }   df_downsampled <- data.frame()   # generate sub-clusters for each cluster   for (i in 1:n_cl) {     cl <- clusterList[i]     logger(sprintf("Subclustering cluster %d...\n", cl))     timeit(" Subcluster kmeans: ",            cl_model <- CustomKMeans(              df[df[, clust_column] == cl, norm_columns],              centers = n_micros, nstart = 10))     df_subcenters <- cl_model$center     scores <- rowMeans(df_subcenters)     df_subcenters <- data.frame(df_subcenters, scores)     colnames(df_subcenters) <- c(subcluster_column_list, subscore_col)     df_subcenters[, clust_column] <- cl     df_subcenters[, temp_scl_col] <- 1:nrow(df_subcenters)     df_subcenters <- df_subcenters[order(df_subcenters[, subscore_col]),]     df_subcenters[, subcl_column] <- 1:nrow(df_subcenters)     df[df[, clust_column] == cl, temp_scl_col] <- cl_model$cluster     df_centers[df_centers[, clust_column] == cl, nrcus_column] <- nrow(       df[df[, clust_column] == cl,])     df_downsampled <- rbind(df_downsampled, df_subcenters)   }   library(Rtsne)   logger("Applying t-SNE...\n")   timeit(" t-SNE ",          rtsne_res <- Rtsne(as.matrix(df_downsampled[, subcluster_column_list]),                             check_duplicates = FALSE,                             pca = TRUE))   df_downsampled[, tsnex_column] <- rtsne_res$Y[, 1]   df_downsampled[, tsney_column] <- rtsne_res$Y[, 2]   timeit("Final Merge ",          df <- merge(df, df_downsampled, by = c(clust_column, temp_scl_col)))   dfa <- data.frame(df_centers[, c(label_column, clust_column, score_column,                                    nrcus_column, centroid_labels)])   t1 <- proc.time()   elapsed <- t1[3] - t0[3]   logger(sprintf("Total processing time %.2f min\n", elapsed / 60))   result_list = list("dataframe" = df, "fields" = micro_fields,                      "f1" = subcluster_column_list[1],                      "f2" = subcluster_column_list[2],                      "f3" = subcluster_column_list[3],                      "seg" = clust_column,                      "norm_columns" = norm_columns)   return(result_list) } FullClustering <- function(df, column_list, n_centers, n_micros,                            log_list, log_all_columns = FALSE)   ##   ## full clustering including normalization   ##   {   norm_columns <- c()   nr_input_fields <- length(column_list)   logger(sprintf("Normalising %d columns...", nr_input_fields))   for (i in 1:nr_input_fields) {     col_name <- column_list[i]     new_col <- paste0("P", i, "_", substr(col_name, 1, 3))     df[, new_col] <- df[, col_name] #copy data     norm_columns <- c(norm_columns, new_col)     is_log = 0     if (length(log_list) >= i)       is_log <- log_list[i]     if ((is_log == 1) || (log_all_columns)) {       below_zero_idxs <- (df[, new_col] <= 0)       EXP_BELOW_ZERO <- FALSE       if (sum(below_zero_idxs) > 0) {         if (EXP_BELOW_ZERO) {           df[below_zero_idxs, new_col] <-             exp(df[below_zero_idxs, new_col]) * 1e-3         } else {           fdelta <- min(df[below_zero_idxs, new_col])           df[below_zero_idxs, new_col] <-             (df[below_zero_idxs, new_col] - fdelta + 1) * 1e-5         }       }       df[, new_col] <- log(df[, new_col])     }     min_c <- min(df[, new_col])     max_c <- max(df[, new_col])     df[, new_col] <- (df[, new_col] - min_c) / (max_c - min_c)   }   clustering_results <- SimpleClustering(df, norm_columns,                                          n_centers = n_centers,                                          n_micros = n_micros)   return(clustering_results) }
results <- FullClustering(df_full[, cluster_fields], cluster_fields,                                n_centers = 4, n_micros = 100,                                log_list = c(0, 1, 1))
results$norm_columns
head(results$dataframe)
results$f1
results$fields
results$segf
results$seg
results$f1
results$norm_columns
head(df_cl)
logger("\nLoading final model...") final_model <- dnn_res_models[[best_model]] logger("Computing full predictions...") yhat_full <- predict(final_model, as.matrix(df_full[, BestFieldsSelection])) Churn.Threshold <- 0.45 pred_full <- as.integer(yhat_full >= Churn.Threshold) nr_churn <- sum(pred_full) logger(sprintf("Total %d churned out of %d customers", nr_churn, nrow(df_full))) logger("Computing confusion matrix...") cnf <- confusionMatrix(pred_full, y_full, positive = "1") print(cnf) df_full["PRED_CHRN"] <- pred_full cluster_fields <- c("R", "F", "M") results <- FullClustering(df_full, cluster_fields,                           n_centers = 4, n_micros = 100,                           log_list = c(0, 1, 1)) df_cl <- results$dataframe normed_cols <- results$norm_columns df_micros <- unique(df_cl[, results$fields]) # plot micros plot1 <- PlotClustering(df_micros,                         field1 = results$f1, field2 = results$f2,                         seg_field = results$seg) plot1 df_churn <- df_cl[df_cl["PRED_CHURN"] == 1,] TestClustering(df_churn[, normed_cols], normed_cols) churn_clust <- SimpleClustering(df_churn, normed_cols,                                 n_centers = 4, n_micros = 25) df_churn_cl <- churn_clust$dataframe df_churn_micros <- unique(df_churn_cl[, churn_clust$fields]) # plot micros + churn plot2 <- PlotClustering(df_churn_micros,                         field1 = churn_clust$f1, field2 = churn_clust$f2,                         seg_field = churn_clust$seg) plot2
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
df_full$PRED
c("nn_full", toString(best_test_layers))
toString(c("nn_full", toString(best_test_layers)))
s <- toString(df_res)
s
write.table(df_res)
s <- capture
s <- capture.output(df_res)
s
print(s)
s2 <- capture.output(conf_mats[[1]])
s2
for(s1 in s2) print(s1)
GetObjectOuput <- function(obj) {   lst <- capture.output(obj)   res <- "\n"   for (s in lst) {     res <- paste0(s, "\n")   }   return(res) }
s <- GetObjectOuput(df_res)
s
lst <- capture.output(df_res)
lst
for(s in lst) print(s)
GetObjectOuput <- function(obj) {   lst <- capture.output(obj)   res <- "\n"   for (s in lst) {     res <- paste0(res, s, "\n")   }   return(res) }
s <- GetObjectOuput(df_res)
s
print(s)
cat(s)
cat(GetObjectOuput(conf_mats[[1]]))
rtvs::debug_source("D:/Dropbox/11. HYPERLOOP/RChurn2/RChurn2/churn2.R")
normed_cols
df_churn[1:10,normed_cols]
df_churn[1:10,]
head(X_full)
nearZeroVar(X_full)
nearZeroVar(X_full, names = TRUE, save)
nearZeroVar(X_full, names = TRUE, saveMetrics = TRUE)
churn_clust <- SimpleClustering(df_churn, normed_cols,                                   n_centers = 4, n_micros = 25)
95/5
timeit("Testing for zero-variance...",   df_nzv <- nearZeroVar(X_full, names = TRUE, saveMetrics = TRUE, allowParallel = TRUE))
row.names(df_nvz$zeroVar==0)
row.names(df_nzv$zeroVar==0)
row.names(df_nzv$zeroVar==TRUE)
row.names(df_nzv[df_nzv$zeroVar==0,])
df_nzv[df_nzv$zeroVar==0,]
df_nzv[df_nzv$zeroVar==TRUE,]
row.names(df_nzv[df_nzv$zeroVar==TRUE,])
zvc <- row.names(df_nzv[df_nzv$zeroVar==TRUE,])
setdiff(Predictor.Fields,zvc)
churn_clust <- SimpleClustering(df_churn, normed_cols,                                   n_centers = 4, n_micros = 25)
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
X_full[1:10,1:10]
min(X_full)
scale(X_full[1:10,1:10])
preProcess(X_full[1:10, 1:10], method = c("center", "scale"))
preprocessor <- preProcess(X_full[1:10, 1:10], method = c("center", "scale"))
predict(preprocessor, X_full[1:10, 1:10])
summary(predict(preprocessor, X_full[1:10, 1:10]))
std(predict(preprocessor, X_full[1:10, 1:10]))
X_full[1:10,1:10]
logger("Preprocessing training...") pp_model <- preProcess(X_train, method = c("center", "scale")) logger("Trasforming training data ...") X_train <- predict(pp_model, X_train) logger("Trasforming testing data ...") X_test <- predict(pp_model, X_test)
colNames
colnames(df_full[,-zero_var_cols])
X_train[1:10,1:10]
X_test[1:10,1:10]
X_full[1:10,1:10]
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
hist
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
hist_list
hist_list[[1]]
length(hist_list)
hist_list[1]
rm(yhat_nn_test) rm(yhat_nn_train) rm(nn_test_layers) Churn.Threshold.List <- c(0.40, 0.45, 0.5) best_train_recall <- 0 best_train_conf <- NULL best_test_recall <- 0 best_test_conf <- NULL best_model <- -1 for (Churn.Threshold in Churn.Threshold.List) {   logger(sprintf("Using Churn.Threshold of %1.2f%%", Churn.Threshold * 100))   for (i in 1:length(dnn_res_test)) {     yh_train <- dnn_res_train[[i]]     yh_test <- dnn_res_test[[i]]     cnn_layers <- dnn_res_layouts[[i]]     cnr_preds <- dnn_res_preds[[i]]     if (FULL_DEBUG) {       logger("Preparing ROC for training ...")       nn_train_roc <- roc(predictor = as.vector(yh_train),                             response = as.vector(y_train))       logger("Preparing ROC for testing...")       nn_test_roc <- roc(predictor = as.vector(yh_test),                             response = as.vector(y_test))       plot(nn_test_roc, main = sprintf("ROC NN layout: [%s]", toString(cnn_layers)))       plot(hist)     }     p_nn_test <- as.numeric(yh_test > Churn.Threshold)     p_nn_train <- as.numeric(yh_train > Churn.Threshold)     nn_res <- GetPredStats(id_model = i, yhat = p_nn_test, y = y_test,                              yhat_train = p_nn_train, y_train = y_train,                              hiddens = cnn_layers)     nn_res["NPRED"] <- cnr_preds     nn_res["THRS"] <- Churn.Threshold     df_res <- rbind(df_res, nn_res)     s <- sprintf("TestRec: %.3f TestKap: %.3f TrainRec: %.3f TrainKap: %.3f on NN ([%s], %.1f thr)",                    nn_res["TestRecall"], nn_res["TestKappa"],                    nn_res["TrainRecall"], nn_res["TrainKappa"],                    toString(cnn_layers), Churn.Threshold)     if (best_train_recall < nn_res["TrainRecall"]) {       best_train_recall <- nn_res["TrainRecall"]       best_train_kappa <- nn_res["TrainKappa"]       best_train_layers <- cnn_layers     }     df     if (best_test_recall < nn_res["TestRecall"]) {       best_test_recall <- nn_res["TestRecall"]       best_test_kappa <- nn_res["TestKappa"]       best_test_layers <- cnn_layers       best_test_conf <- i_conf       best_model <- i       BestFieldsSelection <- dnn_res_fieldslist[[i]]     }     logger(s)     s_results = c(s_results, s)   } } nn_end_time <- Sys.time() nn_time <- difftime(nn_end_time, nn_start_time, units = "sec") logger(sprintf("DNN processing time: %.1f min", nn_time / 60)) logger("Summing results") for (s in s_results) {   logger(s) } #display best train and test - show best test confusion matrix logger(sprintf("Best train Recall: %.3f Kappa: %.3f for %s",                  best_train_recall, best_train_kappa, toString(best_train_layers))) logger(sprintf("Best test  Recall: %.3f Kappa: %.3f for %s",                  best_test_recall, best_test_kappa, toString(best_test_layers))) logger(sprintf("NN Results:\n%s", GetObjectOuput(df_res[order(df_res$TestRecall),]))) logger(sprintf("Best NN Confusion Matrix:\n%s", GetObjectOuput(conf_mats[[best_test_conf]]))) rm(nn_res)
Predictor.Fields
timeit("Testing for zero-variance...",   df_nzv1 <- nearZeroVar(df_full, names = TRUE, saveMetrics = TRUE, allowParallel = TRUE))
df_nzv1
logger("\nRELOADING WITHOUT PREPROCESSING")
train_part
X_full[1:10,1:10]
X_full <- df_full[, Predictor.Fields] y_full <- as.numeric(df_full[, Target.Field])
X_train <- X_full[train_part,] X_test <- X_full[-train_part,] y_train <- y_full[train_part] y_test <- y_full[-train_part]
X_rain[1:10,1:10]
X_train[1:10,1:10]
X_train[1:10,1:10]
X_test[1:10,1:10]
X_full[1:10,1:10]
logger(sprintf(" Train: %d, test %d", nrow(X_train), nrow(X_test))) v_max_depth = 5 v_eta = 0.1 v_gamma = 0.5 v_colsample_bytree = 0.9 v_min_child_weight = 1 v_nrounds = 200 v_subsample = 0.8 IS_QUICK = FALSE if (DEBUG && (Current.Machine.Name == HPC.Machine.Name)) {   max_tests = length(PredictorSelector)   nr_epochs = 10 } else {   max_tests = 1   if (!IS_QUICK)     nr_epochs = 5 else nr_epochs = 1   } ResetConfusionStats() s_results <- c() df_res <- data.frame() RUN_NN <- TRUE if (RUN_NN) {   nn_start_time <- Sys.time()   logger(sprintf("Running a total of %d predictor selection tests for NN",                  max_tests))   dnn_res_train <- list()   dnn_res_test <- list()   dnn_res_preds <- list()   dnn_res_layouts <- list()   dnn_res_models <- list()   dnn_res_fieldslist <- list()   #Check Columns Variability !!!!    Selection.Fields <- Predictor.Fields   hist_list <- list()   for (i in 1:max_tests) {     Selection.Fields <- PredictorSelector[[i]]     nr_preds = length(Selection.Fields)     X_train <- X_full[train_part, Selection.Fields]     X_test <- X_full[-train_part, Selection.Fields]     ##     ##     ##     nr_hid <- ncol(X_train)     logger(sprintf("\nPreparing and training Neural Networks on %d predictors",                    ncol(X_train)))     nn_tests <- Get_DNN_Layouts(nr_hid = nr_hid, QUICK = IS_QUICK)     nr_nn_tests <- length(nn_tests)     logger(sprintf("Running %d tests...", length(nn_tests)))     nn_t = 0     for (nn_test_layers in nn_tests) {       nn_t <- nn_t + 1       logger(sprintf("\nTraining model %d/%d for pred-test %d [Iteration %d/%d]",                      nn_t, nr_nn_tests, i, (i - 1) * nr_nn_tests + nn_t, max_tests * nr_nn_tests))       nr_preds <- ncol(X_train)       nn_clf <- ChurnClassifier(nn_test_layers, nr_preds)       timeit(sprintf("Running fit/training for %d epochs...", nr_epochs),              hist_list[[length(hist_list) + 1]] <- nn_clf %>% fit(x = as.matrix(X_train), y = y_train,                                     batch_size = 512, epochs = nr_epochs, verbose = 1,                                     validation_data = list(as.matrix(X_test), y_test),                                     callbacks = list(callback_early_stopping(patience = 1,                                                                              verbose = 1))              )       )       timeit("Predicting on training data...",              yhat_nn_train <- round(predict(nn_clf, as.matrix(X_train)), digits = 3)       )       timeit("Predicting on testing data...",              yhat_nn_test <- round(predict(nn_clf, as.matrix(X_test)), digits = 3)       )       c_item <- length(dnn_res_train) + 1       dnn_res_train[[c_item]] <- yhat_nn_train       dnn_res_test[[c_item]] <- yhat_nn_test       dnn_res_layouts[[c_item]] <- nn_test_layers       dnn_res_preds[[c_item]] <- ncol(X_train)       dnn_res_models[[c_item]] <- nn_clf       dnn_res_fieldslist[[c_item]] <- Selection.Fields     }   }   # end all NN tests   #print(hist_list)   rm(yhat_nn_test)   rm(yhat_nn_train)   rm(nn_test_layers)   Churn.Threshold.List <- c(0.40, 0.45, 0.5)   best_train_recall <- 0   best_train_conf <- NULL   best_test_recall <- 0   best_test_conf <- NULL   best_model <- -1   for (Churn.Threshold in Churn.Threshold.List) {     logger(sprintf("Using Churn.Threshold of %1.2f%%", Churn.Threshold * 100))     for (i in 1:length(dnn_res_test)) {       yh_train <- dnn_res_train[[i]]       yh_test <- dnn_res_test[[i]]       cnn_layers <- dnn_res_layouts[[i]]       cnr_preds <- dnn_res_preds[[i]]       if (FULL_DEBUG) {         logger("Preparing ROC for training ...")         nn_train_roc <- roc(predictor = as.vector(yh_train),                             response = as.vector(y_train))         logger("Preparing ROC for testing...")         nn_test_roc <- roc(predictor = as.vector(yh_test),                             response = as.vector(y_test))         plot(nn_test_roc, main = sprintf("ROC NN layout: [%s]", toString(cnn_layers)))         plot(hist)       }       p_nn_test <- as.numeric(yh_test > Churn.Threshold)       p_nn_train <- as.numeric(yh_train > Churn.Threshold)       nn_res <- GetPredStats(id_model = i, yhat = p_nn_test, y = y_test,                              yhat_train = p_nn_train, y_train = y_train,                              hiddens = cnn_layers)       nn_res["NPRED"] <- cnr_preds       nn_res["THRS"] <- Churn.Threshold       df_res <- rbind(df_res, nn_res)       s <- sprintf("TestRec: %.3f TestKap: %.3f TrainRec: %.3f TrainKap: %.3f on NN ([%s], %.1f thr)",                    nn_res["TestRecall"], nn_res["TestKappa"],                    nn_res["TrainRecall"], nn_res["TrainKappa"],                    toString(cnn_layers), Churn.Threshold)       if (best_train_recall < nn_res["TrainRecall"]) {         best_train_recall <- nn_res["TrainRecall"]         best_train_kappa <- nn_res["TrainKappa"]         best_train_layers <- cnn_layers       }       df       if (best_test_recall < nn_res["TestRecall"]) {         best_test_recall <- nn_res["TestRecall"]         best_test_kappa <- nn_res["TestKappa"]         best_test_layers <- cnn_layers         best_test_conf <- i_conf         best_model <- i         BestFieldsSelection <- dnn_res_fieldslist[[i]]       }       logger(s)       s_results = c(s_results, s)     }   }   nn_end_time <- Sys.time()   nn_time <- difftime(nn_end_time, nn_start_time, units = "sec")   logger(sprintf("DNN processing time: %.1f min", nn_time / 60))   logger("Summing results")   for (s in s_results) {     logger(s)   }   #display best train and test - show best test confusion matrix   logger(sprintf("Best train Recall: %.3f Kappa: %.3f for %s",                  best_train_recall, best_train_kappa, toString(best_train_layers)))   logger(sprintf("Best test  Recall: %.3f Kappa: %.3f for %s",                  best_test_recall, best_test_kappa, toString(best_test_layers)))   logger(sprintf("NN Results:\n%s", GetObjectOuput(df_res[order(df_res$TestRecall),])))   logger(sprintf("Best NN Confusion Matrix:\n%s", GetObjectOuput(conf_mats[[best_test_conf]])))   rm(nn_res)   ##   ##   ## } else {   logger("Bypassing NN procedure") }
p_nn_test[1:10]
y_test[1:10]
typeof(p_nn_test)
typeof(y_test)
p_nn_train[1:10]
preProcess
ppp <- preProcess(X_train, method = c("range"))
t <- predict(ppp, X_train)
t[1:10,1:10]
X_train[1:10,1:10]
max(X_train$R1)
max(X_train$R_1)
length(BestFieldsSelection)
logger(sprintf("\nLoading final model with layout [%s]...", best_test_layers)) final_model <- dnn_res_models[[best_model]] logger(sprintf("Computing full predictions on data with shape (%s)...",                   toString(dim(df_full[, BestFieldsSelection])))) yhat_full <- predict(final_model, as.matrix(df_full[, BestFieldsSelection])) Churn.Threshold <- 0.45 pred_full <- as.integer(yhat_full >= Churn.Threshold) nr_churn <- sum(pred_full) logger(sprintf("Total %d churned out of %d customers at %.1f threshold",                  nr_churn, nrow(df_full), Churn.Threshold)) logger(sprintf("Computing confusion matrix for threshold %.2f ...", Churn.Threshold)) nnfull_res <- GetPredStats(id_model = 1000, yhat = pred_full, y = y_full,                              hiddens = toString(c("nn_full", toString(best_test_layers)))) nnfull_res["NPRED"] <- length(BestFieldsSelection) nnfull_res["THRS"] <- Churn.Threshold df_res <- rbind(df_res, nnfull_res) s <- sprintf("FullRec: %.3f FullKap: %.3f on NN ([%s], %.1f thr)",                    nnfull_res["TestRecall"], nnfull_res["TestKappa"],                    toString(best_test_layers), Churn.Threshold) logger(sprintf("Final results:\n%s", GetObjectOuput(df_res))) logger(sprintf("Confusion Matrix:\n%s", GetObjectOuput(conf_mats[[i_conf]]))) df_full["PRED_CHURN"] <- pred_full cluster_fields <- c("R", "F", "M") results <- FullClustering(df_full, cluster_fields,                             n_centers = 4, n_micros = 100,                             log_list = c(0, 1, 1)) df_cl <- results$dataframe normed_cols <- results$norm_columns df_micros <- unique(df_cl[, results$fields]) # plot micros plot1 <- PlotClustering(df_micros,                           field1 = results$f1, field2 = results$f2,                           seg_field = results$seg) plot1 df_churn <- df_cl[df_cl["PRED_CHURN"] == 1,] TestClustering(df_churn[, normed_cols], normed_cols)
results <- FullClustering(df_full, cluster_fields,                             n_centers = 4, n_micros = 100,                             log_list = c(0, 1, 1)) df_cl <- results$dataframe normed_cols <- results$norm_columns df_micros <- unique(df_cl[, results$fields]) # plot micros plot1 <- PlotClustering(df_micros,                           field1 = results$f1, field2 = results$f2,                           seg_field = results$seg) plot1 df_churn <- df_cl[df_cl["PRED_CHURN"] == 1,] TestClustering(df_churn[, normed_cols], normed_cols)
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
setup_paralel_env()
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
rtvs::debug_source("D:/Dropbox/HYPERLOOP/RChurn2/RChurn2/churn2.R")
n
df_input[1:10,1:10]
df_input[1:10,1:10]
temp_scl_col
clust_column
dim(df_downsampled)
head(df_downsampled)
colnames(df_input)
n
colnames(df_input)
df_centers[, c(label_column, clust_column, score_column,                                    nrcus_column, centroid_labels)]
n
n
n
n
n
n
